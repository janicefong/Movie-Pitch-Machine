{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "413027a4",
      "metadata": {
        "id": "413027a4"
      },
      "source": [
        "## Storyline Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e9562f",
      "metadata": {
        "id": "f4e9562f"
      },
      "source": [
        "### Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9ca07a93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ca07a93",
        "outputId": "5a3da7fe-d5c9-4f0c-95d8-63030dd6a3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\babym\\anaconda3\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: better-profanity in c:\\users\\babym\\anaconda3\\lib\\site-packages (0.7.0)\n",
            "Requirement already satisfied: emoji in c:\\users\\babym\\anaconda3\\lib\\site-packages (2.14.1)\n",
            "Requirement already satisfied: gensim in c:\\users\\babym\\anaconda3\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: pyLDAvis in c:\\users\\babym\\anaconda3\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\babym\\anaconda3\\lib\\site-packages (1.13.1)\n",
            "Requirement already satisfied: bertopic in c:\\users\\babym\\anaconda3\\lib\\site-packages (0.17.0)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\babym\\anaconda3\\lib\\site-packages (4.1.0)\n",
            "Requirement already satisfied: hdbscan in c:\\users\\babym\\anaconda3\\lib\\site-packages (0.8.40)\n",
            "Requirement already satisfied: umap-learn in c:\\users\\babym\\anaconda3\\lib\\site-packages (0.5.7)\n",
            "Requirement already satisfied: stanza in c:\\users\\babym\\anaconda3\\lib\\site-packages (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.7)\n",
            "Requirement already satisfied: funcy in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pyLDAvis) (69.5.1)\n",
            "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from bertopic) (5.22.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from bertopic) (4.66.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.46.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.26.1)\n",
            "Requirement already satisfied: Pillow in c:\\users\\babym\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from umap-learn) (0.59.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from stanza) (4.25.6)\n",
            "Requirement already satisfied: requests in c:\\users\\babym\\anaconda3\\lib\\site-packages (from stanza) (2.32.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\babym\\anaconda3\\lib\\site-packages (from stanza) (3.2.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\babym\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.42.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\babym\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from requests->stanza) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from requests->stanza) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: gensim in c:\\users\\babym\\anaconda3\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\babym\\anaconda3\\lib\\site-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: scipy in c:\\users\\babym\\anaconda3\\lib\\site-packages (1.13.1)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\babym\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: win-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy better-profanity emoji gensim pyLDAvis scipy bertopic sentence-transformers hdbscan umap-learn stanza\n",
        "%pip install --upgrade gensim numpy scipy\n",
        "!conda install gensim -y\n",
        "\n",
        "# Restart kernel manually or add this to force reload\n",
        "# import os\n",
        "# os._exit(00)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8971b8d9",
      "metadata": {
        "id": "8971b8d9"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7be9ce00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7be9ce00",
        "outputId": "8fa25b6d-900e-485a-df62-39f0a855b9ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\babym\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\babym\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\babym\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\babym\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\babym\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\babym\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "from umap import UMAP\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "#Data Preprocessing and Feature Engineering\n",
        "import re\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "import stanza\n",
        "from better_profanity import profanity\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim_models\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, pipeline\n",
        "import torch\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89195089",
      "metadata": {
        "id": "89195089"
      },
      "source": [
        "### Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5f88e7fa",
      "metadata": {
        "id": "5f88e7fa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3326038816b4718b006a470c01de351",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-01 18:26:02 INFO: Downloaded file to C:\\Users\\babym\\stanza_resources\\resources.json\n",
            "2025-05-01 18:26:02 INFO: Downloading default packages for language: en (English) ...\n",
            "2025-05-01 18:26:04 INFO: File exists: C:\\Users\\babym\\stanza_resources\\en\\default.zip\n",
            "2025-05-01 18:26:12 INFO: Finished downloading models and saved to C:\\Users\\babym\\stanza_resources\n",
            "2025-05-01 18:26:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c150241ebc94696bed6c973ef29faa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-01 18:26:12 INFO: Downloaded file to C:\\Users\\babym\\stanza_resources\\resources.json\n",
            "2025-05-01 18:26:12 WARNING: Language en package default expects mwt, which has been added\n",
            "2025-05-01 18:26:13 INFO: Loading these models for language: en (English):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "=================================\n",
            "\n",
            "2025-05-01 18:26:13 INFO: Using device: cpu\n",
            "2025-05-01 18:26:13 INFO: Loading: tokenize\n",
            "2025-05-01 18:26:13 INFO: Loading: mwt\n",
            "2025-05-01 18:26:13 INFO: Loading: pos\n",
            "2025-05-01 18:26:18 INFO: Loading: lemma\n",
            "2025-05-01 18:26:20 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "filename = 'reddit_comments.csv'\n",
        "output_filename = 'reddit_storyline_output.csv'\n",
        "vis_output = 'lda_visualization_storyline.html'\n",
        "vis_output_bert = 'lda_visualization_storyline_bert.html'\n",
        "num_topics = 50\n",
        "num_words=10\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "# Load Stanza model for English\n",
        "stanza.download(\"en\")\n",
        "nlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,pos,lemma\")\n",
        "\n",
        "# Add custom stop words for generic nouns\n",
        "custom_stop_words = set([\"thing\", \"stuff\", \"person\", \"people\"])\n",
        "auxiliary_verbs = {'be', 'have', 'do', 'will', 'shall', 'would', 'should', 'can', 'could', 'may', 'might', 'must'}\n",
        "\n",
        "# Load Topic Modelling BERT Model and Tokenizer\n",
        "bert_topic_modelling_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_topic_modelling_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load RoBERTa Sentiment Analysis tokenizer and model\n",
        "sentiment_model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
        "\n",
        "# Create sentiment pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_tokenizer)\n",
        "\n",
        "label_map = {\n",
        "    \"LABEL_0\": \"Negative\",\n",
        "    \"LABEL_1\": \"Neutral\",\n",
        "    \"LABEL_2\": \"Positive\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "858c7aa8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "858c7aa8",
        "outputId": "1628e767-4abb-4247-c8a1-c71ab3af8054"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post Title</th>\n",
              "      <th>Post URL</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>Parent ID</th>\n",
              "      <th>Author</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Score</th>\n",
              "      <th>Reddit Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Making Friends Monday! Share your game tags here!</td>\n",
              "      <td>https://www.reddit.com/r/gaming/comments/1jyrv...</td>\n",
              "      <td>mn0q5r5</td>\n",
              "      <td></td>\n",
              "      <td>telking777</td>\n",
              "      <td>2025-04-14 06:06:31</td>\n",
              "      <td>EverestSparrow\\n\\nPlayStation</td>\n",
              "      <td>2</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Making Friends Monday! Share your game tags here!</td>\n",
              "      <td>https://www.reddit.com/r/gaming/comments/1jyrv...</td>\n",
              "      <td>mn1pw56</td>\n",
              "      <td></td>\n",
              "      <td>Midnight_Starligt</td>\n",
              "      <td>2025-04-14 11:57:18</td>\n",
              "      <td>I play a lot of BG3, cyberpunk and Monster Hun...</td>\n",
              "      <td>2</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Making Friends Monday! Share your game tags here!</td>\n",
              "      <td>https://www.reddit.com/r/gaming/comments/1jyrv...</td>\n",
              "      <td>mn2kaqv</td>\n",
              "      <td>mn1pw56</td>\n",
              "      <td>Tiny-Oven6944</td>\n",
              "      <td>2025-04-14 14:57:51</td>\n",
              "      <td>Good choice</td>\n",
              "      <td>1</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Post Title  \\\n",
              "0  Making Friends Monday! Share your game tags here!   \n",
              "1  Making Friends Monday! Share your game tags here!   \n",
              "2  Making Friends Monday! Share your game tags here!   \n",
              "\n",
              "                                            Post URL Comment ID Parent ID  \\\n",
              "0  https://www.reddit.com/r/gaming/comments/1jyrv...    mn0q5r5             \n",
              "1  https://www.reddit.com/r/gaming/comments/1jyrv...    mn1pw56             \n",
              "2  https://www.reddit.com/r/gaming/comments/1jyrv...    mn2kaqv   mn1pw56   \n",
              "\n",
              "              Author            Timestamp  \\\n",
              "0         telking777  2025-04-14 06:06:31   \n",
              "1  Midnight_Starligt  2025-04-14 11:57:18   \n",
              "2      Tiny-Oven6944  2025-04-14 14:57:51   \n",
              "\n",
              "                                             Comment  Score Reddit Name  \n",
              "0                      EverestSparrow\\n\\nPlayStation      2      gaming  \n",
              "1  I play a lot of BG3, cyberpunk and Monster Hun...      2      gaming  \n",
              "2                                        Good choice      1      gaming  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(filename, encoding='utf-8')\n",
        "df.fillna('', inplace=True)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc1b83e",
      "metadata": {
        "id": "4bc1b83e"
      },
      "source": [
        "### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "38cc5acf",
      "metadata": {
        "id": "38cc5acf"
      },
      "outputs": [],
      "source": [
        "#Removing stopwords and words with unusual symbols\n",
        "def text_processing(text):\n",
        "    # Handle non-string inputs\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert emoji to words\n",
        "    text = emoji.demojize(text)\n",
        "\n",
        "    #Generating the list of words in the message (hastags and other punctuations removed) and convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace profanity with asterisks\n",
        "    text = profanity.censor(text)\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\W\\d]*$', '', text)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    normalized_message = []\n",
        "    \n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # Map Stanza's UPOS tags to SpaCy-like tags\n",
        "            if word.upos == \"VERB\" and word.lemma not in auxiliary_verbs:\n",
        "                normalized_message.append(word.lemma)\n",
        "            elif word.upos in [\"NOUN\", \"PROPN\"] and word.lemma not in custom_stop_words:\n",
        "                normalized_message.append(word.lemma)\n",
        "\n",
        "    # Filter out stop words and short tokens\n",
        "    tokens = [token for token in normalized_message if token not in stop_words and len(token) > 2]\n",
        "\n",
        "    # Generate string output\n",
        "    string = ' '.join(tokens) if tokens else \"\"\n",
        "\n",
        "    return (tokens,string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c1f7c10f",
      "metadata": {
        "id": "c1f7c10f"
      },
      "outputs": [],
      "source": [
        "df[['processed_text_lda', 'processed_text_bertopic']] = pd.DataFrame(\n",
        "    df['Comment'].apply(text_processing).tolist(),\n",
        "    index=df.index,\n",
        "    columns=['processed_text_lda', 'processed_text_bertopic']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "mZwYz7Rpjw5B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mZwYz7Rpjw5B",
        "outputId": "4c126a13-cefe-4b26-90aa-66811ec626c0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post Title</th>\n",
              "      <th>Post URL</th>\n",
              "      <th>Comment ID</th>\n",
              "      <th>Parent ID</th>\n",
              "      <th>Author</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Score</th>\n",
              "      <th>Reddit Name</th>\n",
              "      <th>processed_text_lda</th>\n",
              "      <th>processed_text_bertopic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Making Friends Monday! Share your game tags here!</td>\n",
              "      <td>https://www.reddit.com/r/gaming/comments/1jyrv...</td>\n",
              "      <td>mn0q5r5</td>\n",
              "      <td></td>\n",
              "      <td>telking777</td>\n",
              "      <td>2025-04-14 06:06:31</td>\n",
              "      <td>EverestSparrow\\n\\nPlayStation</td>\n",
              "      <td>2</td>\n",
              "      <td>gaming</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Making Friends Monday! Share your game tags here!</td>\n",
              "      <td>https://www.reddit.com/r/gaming/comments/1jyrv...</td>\n",
              "      <td>mn1pw56</td>\n",
              "      <td></td>\n",
              "      <td>Midnight_Starligt</td>\n",
              "      <td>2025-04-14 11:57:18</td>\n",
              "      <td>I play a lot of BG3, cyberpunk and Monster Hun...</td>\n",
              "      <td>2</td>\n",
              "      <td>gaming</td>\n",
              "      <td>[play, lot, cyberpunk, monster, hunter, want, ...</td>\n",
              "      <td>play lot cyberpunk monster hunter want campaig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Post Title  \\\n",
              "0  Making Friends Monday! Share your game tags here!   \n",
              "1  Making Friends Monday! Share your game tags here!   \n",
              "\n",
              "                                            Post URL Comment ID Parent ID  \\\n",
              "0  https://www.reddit.com/r/gaming/comments/1jyrv...    mn0q5r5             \n",
              "1  https://www.reddit.com/r/gaming/comments/1jyrv...    mn1pw56             \n",
              "\n",
              "              Author            Timestamp  \\\n",
              "0         telking777  2025-04-14 06:06:31   \n",
              "1  Midnight_Starligt  2025-04-14 11:57:18   \n",
              "\n",
              "                                             Comment  Score Reddit Name  \\\n",
              "0                      EverestSparrow\\n\\nPlayStation      2      gaming   \n",
              "1  I play a lot of BG3, cyberpunk and Monster Hun...      2      gaming   \n",
              "\n",
              "                                  processed_text_lda  \\\n",
              "0                                                 []   \n",
              "1  [play, lot, cyberpunk, monster, hunter, want, ...   \n",
              "\n",
              "                             processed_text_bertopic  \n",
              "0                                                     \n",
              "1  play lot cyberpunk monster hunter want campaig...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "-ogML8obc_b5",
      "metadata": {
        "id": "-ogML8obc_b5"
      },
      "outputs": [],
      "source": [
        "df.to_csv(output_filename, index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l7nhCpbwfymo",
      "metadata": {
        "id": "l7nhCpbwfymo"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a31eC2vGf1L_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a31eC2vGf1L_",
        "outputId": "497714c3-bf24-408c-9f24-4940d17b7f1d"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'processed_text_bertopic'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\babym\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'processed_text_bertopic'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filter out empty documents\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m valid_docs \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text_bertopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m texts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[valid_docs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text_bertopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      4\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex[valid_docs]\u001b[38;5;241m.\u001b[39mtolist()\n",
            "File \u001b[1;32mc:\\Users\\babym\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\babym\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'processed_text_bertopic'"
          ]
        }
      ],
      "source": [
        "# Filter out empty documents\n",
        "valid_docs = df['processed_text_bertopic'].str.len() > 0\n",
        "texts = df.loc[valid_docs, 'processed_text_bertopic'].tolist()\n",
        "valid_indices = df.index[valid_docs].tolist()\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embeddings(texts, batch_size=16, max_seq_length=128):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = bert_topic_modelling_tokenizer(batch_texts, return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_topic_modelling_model(**inputs)\n",
        "        # Use [CLS] token embedding or mean of token embeddings\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] token\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"Generating BERT embeddings...\")\n",
        "embeddings = get_bert_embeddings(texts)\n",
        "\n",
        "# Step 4: Cluster Embeddings to Identify Topics\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
        "topic_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "bert_topic_words = []\n",
        "# Step 5: Extract Representative Words for Each Topic\n",
        "def get_topic_words(texts, labels, num_words=num_words):\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "    doc_term_matrix = vectorizer.fit_transform(texts)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topic_words = []\n",
        "\n",
        "    for topic_idx in range(num_topics):\n",
        "        topic_docs = [i for i, label in enumerate(labels) if label == topic_idx]\n",
        "        if not topic_docs:\n",
        "            topic_words.append(f\"Topic {topic_idx + 1}: (empty topic)\")\n",
        "            continue\n",
        "        topic_doc_term = doc_term_matrix[topic_docs].sum(axis=0).A1\n",
        "        top_word_indices = topic_doc_term.argsort()[-num_words:][::-1]\n",
        "        wordlist = [feature_names[i] for i in top_word_indices]\n",
        "        bert_topic_words.extend(wordlist)\n",
        "        words = \", \".join(wordlist)\n",
        "        topic_words.append(f\"Topic {topic_idx + 1}: {words}\")\n",
        "\n",
        "    return topic_words\n",
        "\n",
        "# Extract topic words\n",
        "topic_report = get_topic_words(texts, topic_labels)\n",
        "\n",
        "# Save topic report\n",
        "with open(\"topic_modeling_report_bertopic_reddit.txt\", \"w\") as f:\n",
        "    f.write(\"BERTopic Modeling Results\\n\")\n",
        "    f.write(\"=====================\\n\")\n",
        "    for line in topic_report:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Step 7: Assign Topics to Documents\n",
        "dominant_topics = [None] * len(df)\n",
        "for i, (idx, label) in enumerate(zip(valid_indices, topic_labels)):\n",
        "    dominant_topics[idx] = label\n",
        "df['dominant_topic_bert'] = dominant_topics\n",
        "df.to_csv(output_filename, index=False, encoding='utf_8_sig')\n",
        "\n",
        "# Step 8: Visualize Topics with UMAP\n",
        "print(\"Generating topic visualization...\")\n",
        "umap_model = UMAP(n_components=2, random_state=42)\n",
        "umap_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=umap_embeddings[:, 0], y=umap_embeddings[:, 1], hue=topic_labels, palette='deep', s=50)\n",
        "plt.title(\"BERT Topic Clusters (UMAP)\")\n",
        "plt.xlabel(\"UMAP Dimension 1\")\n",
        "plt.ylabel(\"UMAP Dimension 2\")\n",
        "plt.legend(title=\"Topic\")\n",
        "plt.savefig(\"bert_topic_visualization_reddit.png\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"BERT topic modeling completed. Results saved to 'bert_topic_report.txt', 'posts_with_bert_topics.csv', and 'bert_topic_visualization.png'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b64ae1",
      "metadata": {},
      "source": [
        "#### Extract Top 50 words from Top 50 topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "74d37d05",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 Words Across All Topics:\n",
            " 1. make            38\n",
            " 2. think           32\n",
            " 3. time            32\n",
            " 4. game            26\n",
            " 5. say             23\n",
            " 6. know            22\n",
            " 7. want            20\n",
            " 8. feel            17\n",
            " 9. movie           13\n",
            "10. read            12\n",
            "11. play            12\n",
            "12. use             11\n",
            "13. love            11\n",
            "14. look            10\n",
            "15. year            10\n",
            "16. come            8\n",
            "17. like            7\n",
            "18. world           7\n",
            "19. watch           7\n",
            "20. character       6\n",
            "21. way             5\n",
            "22. series          5\n",
            "23. book            5\n",
            "24. story           5\n",
            "25. need            5\n",
            "26. mean            5\n",
            "27. man             4\n",
            "28. film            4\n",
            "29. post            4\n",
            "30. harry           3\n",
            "31. nintendo        3\n",
            "32. point           3\n",
            "33. country         3\n",
            "34. start           3\n",
            "35. end             3\n",
            "36. king            3\n",
            "37. season          3\n",
            "38. work            3\n",
            "39. comment         3\n",
            "40. guy             3\n",
            "41. potter          2\n",
            "42. war             2\n",
            "43. try             2\n",
            "44. let             2\n",
            "45. finish          2\n",
            "46. actor           2\n",
            "47. stephen         2\n",
            "48. episode         2\n",
            "49. sound           2\n",
            "50. day             2\n"
          ]
        }
      ],
      "source": [
        "word_counts = Counter(bert_topic_words)\n",
        "\n",
        "# Get the top 50 words by frequency\n",
        "top_50_words = word_counts.most_common(50)\n",
        "\n",
        "# Print or save the results\n",
        "print(\"Top 50 Words Across All Topics:\")\n",
        "for rank, (word, score) in enumerate(top_50_words, 1):\n",
        "    print(f\"{rank:2}. {word:<15} {score}\")\n",
        "\n",
        "# Save to CSV (optional)\n",
        "top_words_df = pd.DataFrame(top_50_words, columns=[\"Word\", \"Frequency\"])\n",
        "top_words_df.to_csv(\"top_50_words_from_bert_reddit.csv\", index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a4b4daac",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(output_filename, index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WqSoGYvXfsP5",
      "metadata": {
        "id": "WqSoGYvXfsP5"
      },
      "source": [
        "### LDA [Not In use]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7c23e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "6c7c23e7",
        "outputId": "6abd7ec9-741b-42ab-b7cf-e420231af0e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.132*\"magic\" + 0.096*\"style\" + 0.083*\"horror\" + 0.060*\"attack\" + 0.057*\"sword\" + 0.045*\"setting\" + 0.043*\"spot\" + 0.038*\"weapon\" + 0.032*\"trope\" + 0.028*\"rip\"\n",
            "0.207*\"fan\" + 0.100*\"parent\" + 0.095*\"top\" + 0.092*\"class\" + 0.046*\"roll\" + 0.035*\"court\" + 0.026*\"paint\" + 0.025*\"stomach\" + 0.025*\"would\" + 0.025*\"means\"\n",
            "0.110*\"anime\" + 0.075*\"argue\" + 0.067*\"involve\" + 0.049*\"discuss\" + 0.048*\"recommendation\" + 0.048*\"prove\" + 0.046*\"format\" + 0.041*\"suck\" + 0.035*\"twist\" + 0.032*\"middle\"\n",
            "0.190*\"agree\" + 0.130*\"human\" + 0.102*\"sort\" + 0.097*\"figure\" + 0.078*\"value\" + 0.037*\"battle\" + 0.029*\"sale\" + 0.026*\"replace\" + 0.025*\"represent\" + 0.023*\"nonsense\"\n",
            "0.156*\"mind\" + 0.108*\"deal\" + 0.070*\"reminder\" + 0.067*\"recommend\" + 0.061*\"interest\" + 0.051*\"struggle\" + 0.041*\"tend\" + 0.032*\"app\" + 0.031*\"deck\" + 0.024*\"affect\"\n",
            "0.119*\"side\" + 0.083*\"body\" + 0.082*\"send\" + 0.075*\"discussion\" + 0.061*\"taste\" + 0.045*\"wipe\" + 0.038*\"rise\" + 0.034*\"area\" + 0.031*\"manager\" + 0.031*\"article\"\n",
            "0.199*\"reason\" + 0.158*\"follow\" + 0.095*\"age\" + 0.063*\"today\" + 0.059*\"dragon\" + 0.050*\"era\" + 0.043*\"effect\" + 0.042*\"radio\" + 0.029*\"reveal\" + 0.028*\"direction\"\n",
            "0.293*\"day\" + 0.177*\"woman\" + 0.081*\"wear\" + 0.067*\"save\" + 0.050*\"pilot\" + 0.050*\"remind\" + 0.035*\"rate\" + 0.030*\"bar\" + 0.023*\"increase\" + 0.016*\"delete\"\n",
            "0.095*\"job\" + 0.076*\"order\" + 0.069*\"hero\" + 0.061*\"decide\" + 0.059*\"begin\" + 0.051*\"label\" + 0.044*\"draw\" + 0.040*\"marvel\" + 0.033*\"foot\" + 0.033*\"internet\"\n",
            "0.124*\"hate\" + 0.093*\"moment\" + 0.090*\"fight\" + 0.074*\"space\" + 0.070*\"group\" + 0.069*\"camera\" + 0.061*\"strike\" + 0.049*\"hurt\" + 0.047*\"reach\" + 0.039*\"peak\"\n",
            "0.271*\"work\" + 0.104*\"scene\" + 0.059*\"matter\" + 0.058*\"school\" + 0.051*\"hit\" + 0.042*\"reddit\" + 0.034*\"fun\" + 0.030*\"bunch\" + 0.026*\"context\" + 0.023*\"idk\"\n",
            "0.227*\"show\" + 0.192*\"write\" + 0.141*\"comment\" + 0.058*\"guess\" + 0.046*\"relationship\" + 0.036*\"realize\" + 0.029*\"quality\" + 0.026*\"reference\" + 0.024*\"influence\" + 0.018*\"nature\"\n",
            "0.295*\"story\" + 0.172*\"character\" + 0.117*\"part\" + 0.077*\"series\" + 0.053*\"bring\" + 0.050*\"death\" + 0.035*\"choice\" + 0.025*\"worry\" + 0.024*\"attention\" + 0.020*\"beginning\"\n",
            "0.548*\"make\" + 0.076*\"script\" + 0.056*\"sense\" + 0.035*\"industry\" + 0.027*\"step\" + 0.026*\"advice\" + 0.024*\"door\" + 0.016*\"decision\" + 0.016*\"escape\" + 0.013*\"iron\"\n",
            "0.396*\"say\" + 0.178*\"find\" + 0.093*\"hear\" + 0.083*\"kid\" + 0.070*\"turn\" + 0.032*\"finish\" + 0.032*\"sell\" + 0.023*\"demon\" + 0.015*\"hide\" + 0.014*\"community\"\n",
            "0.386*\"see\" + 0.233*\"movie\" + 0.131*\"watch\" + 0.055*\"die\" + 0.032*\"feeling\" + 0.011*\"field\" + 0.011*\"personality\" + 0.009*\"address\" + 0.008*\"spiderman\" + 0.008*\"library\"\n",
            "0.220*\"read\" + 0.208*\"love\" + 0.185*\"like\" + 0.156*\"mean\" + 0.054*\"fact\" + 0.038*\"stand\" + 0.023*\"track\" + 0.018*\"ignore\" + 0.015*\"click\" + 0.013*\"mark\"\n",
            "0.224*\"feel\" + 0.202*\"way\" + 0.101*\"end\" + 0.060*\"friend\" + 0.039*\"detail\" + 0.029*\"couple\" + 0.027*\"month\" + 0.026*\"state\" + 0.026*\"pop\" + 0.025*\"wife\"\n",
            "0.427*\"think\" + 0.109*\"point\" + 0.068*\"idea\" + 0.068*\"play\" + 0.052*\"game\" + 0.034*\"create\" + 0.028*\"video\" + 0.023*\"thread\" + 0.022*\"feature\" + 0.014*\"blow\"\n",
            "0.277*\"get\" + 0.165*\"time\" + 0.140*\"want\" + 0.090*\"try\" + 0.054*\"keep\" + 0.053*\"ask\" + 0.040*\"put\" + 0.023*\"set\" + 0.019*\"feedback\" + 0.018*\"spend\"\n",
            "Topic modeling completed.\n"
          ]
        }
      ],
      "source": [
        "# Create Document-Term Matrix\n",
        "# Filter out empty documents and keep track of valid indices\n",
        "valid_docs_lda = df['processed_text_lda'].str.strip() != ''\n",
        "processed_docs_lda = df.loc[valid_docs_lda, 'processed_text_lda'].tolist()\n",
        "valid_indices_lda = df.index[valid_docs_lda].tolist()\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(processed_docs_lda)\n",
        "\n",
        "# Create corpus (bag-of-words representation)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs_lda]\n",
        "\n",
        "# Train LDA model\n",
        "lda_model = LdaModel(corpus=corpus, # Create LDA model\n",
        "                     id2word=dictionary, # Dictionary for the model\n",
        "                     num_topics=num_topics, # Number of topics\n",
        "                     random_state=42, # Random state for reproducibility\n",
        "                     passes=10, # Number of passes through the corpus\n",
        "                     alpha='auto', # Hyperparameter for document-topic density\n",
        "                     eta='auto') # Hyperparameter for topic-word density\n",
        "\n",
        "# Extract topics and their words\n",
        "topics_lda = lda_model.print_topics(num_words=10)\n",
        "topic_report_lda = []\n",
        "lda_topic_words = []\n",
        "for idx, topic in topics_lda:\n",
        "    topic_words_processed = [word.split(\"*\")[1].strip('\" ') for word in topic[1].split(\" + \")]\n",
        "    lda_topic_words.extend(topic_words_processed)\n",
        "    topic_words = \", \".join(topic_words_processed)\n",
        "    topic_report_lda.append(f\"Topic {idx + 1}: {topic_words}\")\n",
        "\n",
        "# Get dominate topic\n",
        "def get_dominant_topic(doc_bow):\n",
        "    topic_dist = lda_model[doc_bow]\n",
        "    if topic_dist:\n",
        "        return max(topic_dist, key=lambda x: x[1])[0]\n",
        "    return None\n",
        "\n",
        "with open(\"topic_modeling_report_lda_reddit.txt\", \"w\") as f:\n",
        "    f.write(\"Topic Modeling Results\\n\")\n",
        "    f.write(\"=====================\\n\")\n",
        "    for line in topic_report_lda:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "dominant_topics_lda = [None] * len(df)\n",
        "for i, (idx, bow) in enumerate(zip(valid_indices_lda, corpus)):\n",
        "    dominant_topics_lda[idx] = get_dominant_topic(bow)\n",
        "\n",
        "df['dominant_topic_lda'] = dominant_topics_lda\n",
        "df.to_csv(output_filename, index=False, encoding='utf_8_sig')\n",
        "\n",
        "# Visualize Topics with pyLDAvis\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.save_html(vis, vis_output) # Save the visualization to an HTML file\n",
        "\n",
        "print(\"Topic modeling completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "220444be",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(output_filename, index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611e6221",
      "metadata": {},
      "source": [
        "#### Extract Top 50 words from Top 50 topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "89588fe1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 50 Words Across All Topics:\n"
          ]
        }
      ],
      "source": [
        "word_counts = Counter(lda_topic_words)\n",
        "\n",
        "# Get the top 50 words by frequency\n",
        "top_50_words = word_counts.most_common(50)\n",
        "\n",
        "# Print or save the results\n",
        "print(\"Top 50 Words Across All Topics:\")\n",
        "for rank, (word, score) in enumerate(top_50_words, 1):\n",
        "    print(f\"{rank:2}. {word:<15} {score}\")\n",
        "\n",
        "# Save to CSV (optional)\n",
        "top_words_df = pd.DataFrame(top_50_words, columns=[\"Word\", \"Frequency\"])\n",
        "top_words_df.to_csv(\"top_50_words_from_lda_reddit.csv\", index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01eb836a",
      "metadata": {},
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e0c8551",
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = \"The story was amazing and emotional!\"\n",
        "# result = sentiment_pipeline(text)[0]\n",
        "# label = label_map[result['label']]\n",
        "# score = result['score']\n",
        "# print(f\"Sentiment: {label}, Score: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "346450cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentiment(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return pd.Series([None, None])\n",
        "\n",
        "    # Add truncation=True\n",
        "    result = sentiment_pipeline(text, truncation=True, max_length=512)[0]\n",
        "\n",
        "    return pd.Series([label_map[result['label']], result['score']])\n",
        "\n",
        "# Add sentiment results to DataFrame\n",
        "df[['sentiment_label', 'sentiment_score']] = df['Comment'].apply(get_sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1b3e53a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(output_filename, index=False, encoding='utf_8_sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1a6f79",
      "metadata": {},
      "outputs": [],
      "source": [
        "word_counts = Counter(lda_topic_words)\n",
        "\n",
        "# Get the top 50 words by frequency\n",
        "top_50_words = word_counts.most_common(50)\n",
        "\n",
        "# Print or save the results\n",
        "print(\"Top 50 Words Across All Topics:\")\n",
        "for rank, (word, score) in enumerate(top_50_words, 1):\n",
        "    print(f\"{rank:2}. {word:<15} {score}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
